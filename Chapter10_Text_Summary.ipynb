{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter10: Text Summary.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYHDQYZ-uMBI"
      },
      "source": [
        "#Import packages \n",
        "\n",
        "#data processing \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#For text extraction\n",
        "import re         \n",
        "from bs4 import BeautifulSoup \n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "\n",
        "#For modeling\n",
        "#import tf\n",
        "import tensorflow as tf\n",
        "\n",
        "# import keras\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6isoYhDJuTCp"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI39qPRluTGy"
      },
      "source": [
        "#Importing data\n",
        "\n",
        "df=pd.read_csv('sum1.csv')    \n",
        "df.head()\n",
        "\n",
        "# Rows and columns in the dataset\n",
        "df.shape\n",
        "\n",
        "# Check for any null values\n",
        "df.isnull().sum()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VOzyYZYupd9"
      },
      "source": [
        "# Text pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83K05-iTuTKx"
      },
      "source": [
        "#Text preprocessing function\n",
        "\n",
        "def txt_preprocessing(txt): \n",
        "    txt = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', txt)\n",
        "    txt = re.sub(r'\\'', ' ', txt)\n",
        "    txt = txt.lower()\n",
        "    return txt\n",
        "\n",
        "  \n",
        "headlines_processed = []\n",
        "clean_news = []\n",
        "\n",
        "# headline text cleaning\n",
        "for z in df.headlines:\n",
        "    headlines_processed.append(txt_preprocessing(z))\n",
        "\n",
        "\n",
        "# news text cleaning\n",
        "for z in df.text:\n",
        "    clean_news.append(txt_preprocessing(z))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Jgpshcu3D7"
      },
      "source": [
        "# Simple Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzYH76-muTN5"
      },
      "source": [
        "#connect to hugging face\n",
        "!git clone https://github.com/huggingface/transformers \\\n",
        "&& cd transformers \\\n",
        "\n",
        "#install\n",
        "!pip install -q ./transformers\n",
        "\n",
        "#Import\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
        "\n",
        "torch_device = 'cpu'\n",
        "\n",
        "#Function to summarize\n",
        "def summarize_news(input_text, maximum_length, minimum_length):\n",
        "\n",
        "  #Tokenize\n",
        "  input_txt_ids = tokenizer.batch_encode_plus([input_text], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
        "  \n",
        "  #summarize\n",
        "  ids_sum = model.generate(input_txt_ids, max_length=int(maximum_length), min_length=int(minimum_length))      \n",
        "\n",
        "  #get the text summary    \n",
        "  output_sum = tokenizer.decode(ids_sum.squeeze(), skip_special_tokens=True)\n",
        "  return output_sum\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsG7SAJEu0Mb"
      },
      "source": [
        "input_text = \"According to a Hindustan Times report, the 14 Keralites were among the terrorists and militants freed by the Taliban from Bagram jail. As of now, unconfirmed reports state that two Pakistani residents were detained by the Sunni Pashtun terrorist group for trying to blow off an Improvised Explosive Device (IED) device outside Turkmenistan embassy in Kabul on August 26. And as intelligence reports indicate, an IED was recovered from the two Pakistani nationals soon after the Kabul airport blast. As per reports, a Kerala resident contacted his home, while the remaining 13 are still in Kabul with the ISIS-K terrorist group. After Syria and Levant occupied Mosul in 2014, people from the Malappuram, Kasaragod and Kannur districts left India and joined the jihadist group in West Asia from where a few Keralites came down to Nangarhar province of Afghanistan.\"\n",
        "\n",
        "#Generate summary using function\n",
        "summarize_news(input_text,20,10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLDEGmEtyUJO"
      },
      "source": [
        "# Transfer learning: Custom training on top of pre-trained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95uYsH3KyXYP"
      },
      "source": [
        "# BART: Simple-Transformer Pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFlmkuqJu0Qh"
      },
      "source": [
        "#Installing transformers\n",
        "!pip install simpletransformers \n",
        "!pip install transformers \n",
        "\n",
        "#Importing necessary libraries from simple transformers \n",
        "import pandas as pd\n",
        "from simpletransformers.seq2seq import Seq2SeqModel,Seq2SeqArgs \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsEMBqwEu0ZP"
      },
      "source": [
        "#Rename columns as per pretrained model required format\n",
        "df=df.rename(columns={'headlines':'target_text','text':'input_text'})  \n",
        "\n",
        "model_args = Seq2SeqArgs()   \n",
        "\n",
        "#Initializing number of epochs \n",
        "model_args.num_train_epochs = 1\n",
        "\n",
        "#Initializing no_save arg  \n",
        "model_args.no_save = True\n",
        "\n",
        "#Initializing evaluation args  \n",
        "model_args.evaluate_generated_text = True\n",
        "model_args.evaluate_during_training = True\n",
        "model_args.evaluate_during_training_verbose = True \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZOyFl4v3pa9"
      },
      "source": [
        "# Initialize the model with type as 'bart' and provide model args\n",
        "\n",
        "model = Seq2SeqModel(\n",
        "    encoder_decoder_type=\"bart\",\n",
        "    encoder_decoder_name=\"facebook/bart-large\",\n",
        "    args=model_args,\n",
        "    use_cuda=True,\n",
        ")\n",
        "\n",
        " #Splitting data into train-test\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "train_df.shape, test_df.shape\n",
        "\n",
        "#Training the model and keeping eval dataset as test data\n",
        "\n",
        "model.train_model(train_df, eval_data=test_df) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOOJey-Du0hQ"
      },
      "source": [
        "#Generating summaries on news test data\n",
        "results = model.eval_model(test_df)  \n",
        "\n",
        "#print the loss\n",
        "results \n",
        "\n",
        "#Original test data text summary for top 10 news\n",
        "\n",
        "for i in test_df.target_text[:10]:\n",
        "  print(i)   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szBKd9isuTRQ"
      },
      "source": [
        "#Predicted summary\n",
        "for i in test_df.input_text:    \n",
        "  print(model.predict([i]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3_d6aTrzCSR"
      },
      "source": [
        "# T5 Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qagnY5YAzEKi"
      },
      "source": [
        "#Installing simple-T5\n",
        "! pip install simplet5 -q   \n",
        "\n",
        "#Import the library\n",
        "from simplet5 import SimpleT5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3FYkh9YzEOW"
      },
      "source": [
        "# Model expects dataframe to have 2 column names\n",
        "# Input as \"source_text\", Summary as \"target_text\", let us rename accordingly\n",
        "\n",
        "df = df.rename(columns={\"headlines\":\"target_text\", \"input_text\":\"source_text\"})   \n",
        "df = df[['source_text', 'target_text']]\n",
        "\n",
        "# Let us add a prefix \"summarize: \" for all source text\n",
        "\n",
        "df['source_text'] = \"summarize: \" + df['source_text']\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZqDwgKg4Duu"
      },
      "source": [
        "#Splitting data into train and test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "train_df.shape, test_df.shape\n",
        "\n",
        "#Initializing the model\n",
        "model = SimpleT5()   \n",
        "\n",
        "#Importing pretrained t5 model\n",
        "model.from_pretrained(model_type=\"t5\", model_name=\"t5-base\")    \n",
        "\n",
        "#Import torch as this model is built on top of pytorch\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#Training the model with 5 epochs \n",
        "\n",
        "model.train(train_df=train_df,\n",
        "            eval_df=test_df, \n",
        "            source_max_token_len=128, \n",
        "            target_max_token_len=50, \n",
        "            batch_size=8, max_epochs=5, use_gpu=True)\n",
        "\n",
        "#Models built at each epoch\n",
        "! ( cd outputs; ls )   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX8mj-w9zRT9"
      },
      "source": [
        "#Loading model saved with lowest loss\n",
        "model.load_model(\"t5\",\"outputs/SimpleT5-epoch-4-train-loss-0.902\", use_gpu=True)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75hu3sm_zRYw"
      },
      "source": [
        "#Original headlines\n",
        "\n",
        "test_df['target_text'] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iuqgaHYzRdO"
      },
      "source": [
        "#Model summarized headlines\n",
        "for doc in test_df['source_text']:\n",
        "  print(model.predict(doc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwR7LVaczddF"
      },
      "source": [
        "BLUE Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KVn-GZlzEZE"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "x= [x for x in df.source_text]\n",
        "\n",
        "y=[model.predict(p)[0] for p in df['source_text']]\n",
        "\n",
        "# Function to calculate the score\n",
        "L=0\n",
        "for i,j in zip(x,y):\n",
        "  L+=sentence_bleu(\n",
        "    [i],\n",
        "    j,\n",
        "    weights=(0.25, 0.25, 0.25, 0.25),\n",
        "    smoothing_function=None,\n",
        "    auto_reweigh=False,\n",
        ")\n",
        "\n",
        "#Average blue score of whole corpuses\n",
        "L/df.shape[0]   \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}