{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b966179",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873ef83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "import xgboost\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, numpy, textblob, string\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea396e43",
   "metadata": {},
   "source": [
    "# Reading the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6e2718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMOTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fear</td>\n",
       "      <td>Every time I imagine that someone I love or I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I had been obviously unjustly treated and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I think about the short time that we live...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "      <td>At a gathering I found myself involuntarily si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shame</td>\n",
       "      <td>When I realized that I was directing the feeli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMOTION                                               TEXT\n",
       "0     fear  Every time I imagine that someone I love or I ...\n",
       "1    anger  When I had been obviously unjustly treated and...\n",
       "2  sadness  When I think about the short time that we live...\n",
       "3  disgust  At a gathering I found myself involuntarily si...\n",
       "4    shame  When I realized that I was directing the feeli..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('DATA.csv')##########################3\n",
    "data.columns =['EMOTION', 'TEXT']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c2a34",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4214fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TEXT'] = data['TEXT'].apply(lambda a: \" \".join(a.lower() for a in a.split()))\n",
    "data['TEXT'] = data['TEXT'].apply(lambda a: \" \".join(a.replace('[^\\w\\s]','') for a in a.split()))\n",
    "stop = stopwords.words('english')\n",
    "data['TEXT'] = data['TEXT'].apply(lambda a: \" \".join(a for a in a.split() if a not in stop))\n",
    "#data['TEXT'] = data['TEXT'].apply(lambda a: str(TextBlob(a).correct()))\n",
    "st = PorterStemmer()\n",
    "data['TEXT'] =  data['TEXT'].apply(lambda a: \" \".join([st.stem(word) for word in a.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4fe3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMOTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fear</td>\n",
       "      <td>everi time imagin someon love could contact á ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>obvious unjustli treat possibl á elucid this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>think short time live relat á period life thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "      <td>gather found involuntarili sit next two á peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shame</td>\n",
       "      <td>realiz direct feel discont á partner way tri p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMOTION                                               TEXT\n",
       "0     fear  everi time imagin someon love could contact á ...\n",
       "1    anger      obvious unjustli treat possibl á elucid this.\n",
       "2  sadness  think short time live relat á period life thin...\n",
       "3  disgust  gather found involuntarili sit next two á peop...\n",
       "4    shame  realiz direct feel discont á partner way tri p..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febc6c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shame      1094\n",
       "disgust    1094\n",
       "sadness    1094\n",
       "anger      1094\n",
       "fear       1093\n",
       "guilt      1091\n",
       "joy        1091\n",
       "Name: EMOTION, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['EMOTION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75978b76",
   "metadata": {},
   "source": [
    "# Encoding the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e73c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "object = preprocessing.LabelEncoder()\n",
    "data['EMOTION'] = object.fit_transform(data['EMOTION'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b15f3564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1094\n",
       "1    1094\n",
       "5    1094\n",
       "6    1094\n",
       "2    1093\n",
       "4    1091\n",
       "3    1091\n",
       "Name: EMOTION, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['EMOTION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb6525",
   "metadata": {},
   "source": [
    "# Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95eb175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = model_selection.train_test_split(data['TEXT'], data['EMOTION'],stratify= data['EMOTION'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e06de",
   "metadata": {},
   "source": [
    "# converting text to features using countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa12601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(data['TEXT'])\n",
    "cv_xtrain =  cv.transform(Xtrain)\n",
    "cv_xtest =  cv.transform(Xtest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9b142",
   "metadata": {},
   "source": [
    "# Converting text to features using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab50651",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer()\n",
    "tv.fit(data['TEXT'])\n",
    "tv_xtrain =  tv.transform(Xtrain)\n",
    "tv_xtest =  tv.transform(Xtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a52192",
   "metadata": {},
   "source": [
    "# Model build function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6be9c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(model_initializer, independent_variables_training, target, independent_variable_test):#########################\n",
    "    # fit \n",
    "        classifier_model=model_initializer.fit(independent_variables_training, target)###########################3\n",
    "    # predict \n",
    "    modelPred=classifier_model.predict(independent_variable_test)\n",
    "    return metrics.accuracy_score(modelPred, Ytest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70284ba",
   "metadata": {},
   "source": [
    "# NavieBayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d766d16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5368531102979613\n",
      "0.5373758494511239\n"
     ]
    }
   ],
   "source": [
    "# for CV\n",
    "output = build(naive_bayes.MultinomialNB(), cv_xtrain, Ytrain, cv_xtest)\n",
    "print(output)\n",
    "# for TF-IDF\n",
    "output = build(naive_bayes.MultinomialNB(), tv_xtrain, Ytrain, tv_xtest)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f35a0",
   "metadata": {},
   "source": [
    "# Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82e5b060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akskulka/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5405122843700993\n",
      "0.5541035023523262\n"
     ]
    }
   ],
   "source": [
    "# for CV\n",
    "output = build(linear_model.LogisticRegression(), cv_xtrain, Ytrain, cv_xtest)\n",
    "print(output)\n",
    "\n",
    "# for TF-IDF\n",
    "output = build(linear_model.LogisticRegression(), tv_xtrain, Ytrain, tv_xtest)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24130ea",
   "metadata": {},
   "source": [
    "# SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52d6c2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5133298484056456\n",
      "0.553058024046001\n"
     ]
    }
   ],
   "source": [
    "#for cv\n",
    "output = build(svm.SVC(), cv_xtrain, Ytrain, cv_xtest)\n",
    "print(output)\n",
    "\n",
    "#for TF-IDF \n",
    "output = build(svm.SVC(), tv_xtrain, Ytrain, tv_xtest)##########################(),\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098a944",
   "metadata": {},
   "source": [
    "# Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc41df32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5196027182435965\n",
      "0.5373758494511239\n"
     ]
    }
   ],
   "source": [
    "#for CV\n",
    "output = build(ensemble.RandomForestClassifier(), cv_xtrain, Ytrain, cv_xtest)\n",
    "print(output)\n",
    "\n",
    "#for TF-IDF \n",
    "output = build(ensemble.RandomForestClassifier(), tv_xtrain, Ytrain, tv_xtest)#############tv_xtest\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda02e84",
   "metadata": {},
   "source": [
    "# Building Logistic Model using TF-IDF as features as the accuracy is better compared to other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f607e7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.44      0.44       274\n",
      "           1       0.54      0.60      0.57       273\n",
      "           2       0.67      0.67      0.67       273\n",
      "           3       0.46      0.42      0.44       273\n",
      "           4       0.64      0.73      0.68       273\n",
      "           5       0.62      0.53      0.57       273\n",
      "           6       0.50      0.49      0.50       274\n",
      "\n",
      "    accuracy                           0.55      1913\n",
      "   macro avg       0.55      0.55      0.55      1913\n",
      "weighted avg       0.55      0.55      0.55      1913\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression().fit(tv_xtrain, Ytrain)############3tv_xtrain\n",
    "val_predictions = classifier.predict(tv_xtest)\n",
    "\n",
    "# Precision , Recall , F1 - score , Support\n",
    "y_true, y_pred = Ytest, val_predictions\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d39036",
   "metadata": {},
   "source": [
    "# Extracting the twitter data for the emotion predictions and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d644822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search phrase = samsung\n",
      "search limit (start/stop): 2021-11-05 23:59:59\n",
      "search limit (start/stop): 2021-11-04 23:59:59\n",
      "max id (starting point) = 1456773321316397063\n",
      "since id (ending point) = 1456410933450973190\n",
      "count = 1\n",
      "found 100 tweets\n",
      "count = 2\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 3\n",
      "found 100 tweets\n",
      "count = 4\n",
      "found 100 tweets\n",
      "count = 5\n",
      "found 100 tweets\n",
      "count = 6\n",
      "found 100 tweets\n",
      "count = 7\n",
      "found 100 tweets\n",
      "count = 8\n",
      "found 100 tweets\n",
      "count = 9\n",
      "found 100 tweets\n",
      "count = 10\n",
      "found 100 tweets\n",
      "count = 11\n",
      "found 100 tweets\n",
      "count = 12\n",
      "found 100 tweets\n",
      "count = 13\n",
      "found 100 tweets\n",
      "count = 14\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 15\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 16\n",
      "found 100 tweets\n",
      "count = 17\n",
      "found 100 tweets\n",
      "count = 18\n",
      "found 100 tweets\n",
      "count = 19\n",
      "found 100 tweets\n",
      "count = 20\n",
      "found 100 tweets\n",
      "count = 21\n",
      "found 100 tweets\n",
      "count = 22\n",
      "found 100 tweets\n",
      "count = 23\n",
      "found 100 tweets\n",
      "count = 24\n",
      "found 100 tweets\n",
      "count = 25\n",
      "found 100 tweets\n",
      "count = 26\n",
      "found 100 tweets\n",
      "count = 27\n",
      "found 100 tweets\n",
      "count = 28\n",
      "found 100 tweets\n",
      "count = 29\n",
      "found 100 tweets\n",
      "count = 30\n",
      "found 100 tweets\n",
      "count = 31\n",
      "found 100 tweets\n",
      "count = 32\n",
      "found 100 tweets\n",
      "count = 33\n",
      "found 100 tweets\n",
      "count = 34\n",
      "found 100 tweets\n",
      "count = 35\n",
      "found 100 tweets\n",
      "count = 36\n",
      "found 100 tweets\n",
      "count = 37\n",
      "found 100 tweets\n",
      "count = 38\n",
      "found 100 tweets\n",
      "count = 39\n",
      "found 100 tweets\n",
      "count = 40\n",
      "found 100 tweets\n",
      "count = 41\n",
      "found 100 tweets\n",
      "count = 42\n",
      "found 100 tweets\n",
      "count = 43\n",
      "found 100 tweets\n",
      "count = 44\n",
      "found 100 tweets\n",
      "count = 45\n",
      "found 100 tweets\n",
      "count = 46\n",
      "found 100 tweets\n",
      "count = 47\n",
      "found 100 tweets\n",
      "count = 48\n",
      "found 100 tweets\n",
      "count = 49\n",
      "found 100 tweets\n",
      "count = 50\n",
      "found 100 tweets\n",
      "count = 51\n",
      "found 100 tweets\n",
      "count = 52\n",
      "found 100 tweets\n",
      "count = 53\n",
      "found 100 tweets\n",
      "count = 54\n",
      "found 100 tweets\n",
      "count = 55\n",
      "found 100 tweets\n",
      "count = 56\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 57\n",
      "found 100 tweets\n",
      "count = 58\n",
      "found 100 tweets\n",
      "count = 59\n",
      "found 100 tweets\n",
      "count = 60\n",
      "found 100 tweets\n",
      "count = 61\n",
      "found 100 tweets\n",
      "count = 62\n",
      "found 100 tweets\n",
      "count = 63\n",
      "found 100 tweets\n",
      "count = 64\n",
      "found 100 tweets\n",
      "count = 65\n",
      "found 100 tweets\n",
      "count = 66\n",
      "found 100 tweets\n",
      "count = 67\n",
      "found 100 tweets\n",
      "count = 68\n",
      "found 100 tweets\n",
      "count = 69\n",
      "found 100 tweets\n",
      "count = 70\n",
      "found 100 tweets\n",
      "count = 71\n",
      "found 100 tweets\n",
      "count = 72\n",
      "found 97 tweets\n",
      "found 3 tweets\n",
      "count = 73\n",
      "found 100 tweets\n",
      "count = 74\n",
      "found 100 tweets\n",
      "count = 75\n",
      "found 100 tweets\n",
      "count = 76\n",
      "found 100 tweets\n",
      "count = 77\n",
      "found 100 tweets\n",
      "count = 78\n",
      "found 100 tweets\n",
      "count = 79\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 80\n",
      "found 100 tweets\n",
      "count = 81\n",
      "found 100 tweets\n",
      "count = 82\n",
      "found 100 tweets\n",
      "count = 83\n",
      "found 100 tweets\n",
      "count = 84\n",
      "found 100 tweets\n",
      "count = 85\n",
      "found 100 tweets\n",
      "count = 86\n",
      "found 100 tweets\n",
      "count = 87\n",
      "found 100 tweets\n",
      "count = 88\n",
      "found 100 tweets\n",
      "count = 89\n",
      "found 100 tweets\n",
      "count = 90\n",
      "found 100 tweets\n",
      "count = 91\n",
      "found 100 tweets\n",
      "count = 92\n",
      "found 100 tweets\n",
      "count = 93\n",
      "found 100 tweets\n",
      "count = 94\n",
      "found 100 tweets\n",
      "count = 95\n",
      "found 100 tweets\n",
      "count = 96\n",
      "found 100 tweets\n",
      "count = 97\n",
      "found 100 tweets\n",
      "count = 98\n",
      "found 100 tweets\n",
      "count = 99\n",
      "found 100 tweets\n",
      "count = 100\n",
      "found 100 tweets\n",
      "count = 101\n",
      "found 100 tweets\n",
      "count = 102\n",
      "found 100 tweets\n",
      "count = 103\n",
      "found 100 tweets\n",
      "count = 104\n",
      "found 100 tweets\n",
      "count = 105\n",
      "found 100 tweets\n",
      "count = 106\n",
      "found 100 tweets\n",
      "count = 107\n",
      "found 100 tweets\n",
      "count = 108\n",
      "found 100 tweets\n",
      "count = 109\n",
      "found 100 tweets\n",
      "count = 110\n",
      "found 100 tweets\n",
      "count = 111\n",
      "found 100 tweets\n",
      "count = 112\n",
      "found 100 tweets\n",
      "count = 113\n",
      "found 100 tweets\n",
      "count = 114\n",
      "found 100 tweets\n",
      "count = 115\n",
      "found 100 tweets\n",
      "count = 116\n",
      "found 100 tweets\n",
      "count = 117\n",
      "found 100 tweets\n",
      "count = 118\n",
      "found 100 tweets\n",
      "count = 119\n",
      "found 100 tweets\n",
      "count = 120\n",
      "found 100 tweets\n",
      "count = 121\n",
      "found 100 tweets\n",
      "count = 122\n",
      "found 100 tweets\n",
      "count = 123\n",
      "found 100 tweets\n",
      "count = 124\n",
      "found 100 tweets\n",
      "count = 125\n",
      "found 100 tweets\n",
      "count = 126\n",
      "found 100 tweets\n",
      "count = 127\n",
      "found 100 tweets\n",
      "count = 128\n",
      "found 100 tweets\n",
      "count = 129\n",
      "found 100 tweets\n",
      "count = 130\n",
      "found 100 tweets\n",
      "count = 131\n",
      "found 100 tweets\n",
      "count = 132\n",
      "found 100 tweets\n",
      "count = 133\n",
      "found 100 tweets\n",
      "count = 134\n",
      "found 100 tweets\n",
      "count = 135\n",
      "found 95 tweets\n",
      "found 3 tweets\n",
      "found 1 tweets\n",
      "found 0 tweets\n",
      "no tweets found\n",
      "count = 136\n",
      "found 100 tweets\n",
      "count = 137\n",
      "found 100 tweets\n",
      "count = 138\n",
      "found 100 tweets\n",
      "count = 139\n",
      "found 100 tweets\n",
      "count = 140\n",
      "found 100 tweets\n",
      "count = 141\n",
      "found 100 tweets\n",
      "count = 142\n",
      "found 100 tweets\n",
      "count = 143\n",
      "found 100 tweets\n",
      "count = 144\n",
      "found 100 tweets\n",
      "count = 145\n",
      "found 100 tweets\n",
      "count = 146\n",
      "found 100 tweets\n",
      "count = 147\n",
      "found 100 tweets\n",
      "count = 148\n",
      "found 100 tweets\n",
      "count = 149\n",
      "found 100 tweets\n",
      "count = 150\n",
      "found 100 tweets\n",
      "count = 151\n",
      "found 100 tweets\n",
      "count = 152\n",
      "found 100 tweets\n",
      "count = 153\n",
      "found 100 tweets\n",
      "count = 154\n",
      "found 100 tweets\n",
      "count = 155\n",
      "found 100 tweets\n",
      "count = 156\n",
      "found 100 tweets\n",
      "count = 157\n",
      "found 100 tweets\n",
      "count = 158\n",
      "found 100 tweets\n",
      "count = 159\n",
      "found 100 tweets\n",
      "count = 160\n",
      "found 100 tweets\n",
      "count = 161\n",
      "found 100 tweets\n",
      "count = 162\n",
      "found 100 tweets\n",
      "count = 163\n",
      "found 100 tweets\n",
      "count = 164\n",
      "found 100 tweets\n",
      "count = 165\n",
      "found 100 tweets\n",
      "count = 166\n",
      "found 100 tweets\n",
      "count = 167\n",
      "found 100 tweets\n",
      "count = 168\n",
      "found 100 tweets\n",
      "count = 169\n",
      "found 100 tweets\n",
      "count = 170\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2021-11-11 20:18:57.923407 )\n",
      "count = 171\n",
      "found 100 tweets\n",
      "count = 172\n",
      "found 100 tweets\n",
      "count = 173\n",
      "found 100 tweets\n",
      "count = 174\n",
      "found 100 tweets\n",
      "count = 175\n",
      "found 100 tweets\n",
      "count = 176\n",
      "found 100 tweets\n",
      "count = 177\n",
      "found 100 tweets\n",
      "count = 178\n",
      "found 100 tweets\n",
      "count = 179\n",
      "found 100 tweets\n",
      "count = 180\n",
      "found 100 tweets\n",
      "count = 181\n",
      "found 100 tweets\n",
      "count = 182\n",
      "found 100 tweets\n",
      "count = 183\n",
      "found 100 tweets\n",
      "count = 184\n",
      "found 100 tweets\n",
      "count = 185\n",
      "found 100 tweets\n",
      "count = 186\n",
      "found 100 tweets\n",
      "count = 187\n",
      "found 100 tweets\n",
      "count = 188\n",
      "found 100 tweets\n",
      "count = 189\n",
      "found 100 tweets\n",
      "count = 190\n",
      "found 100 tweets\n",
      "count = 191\n",
      "found 100 tweets\n",
      "count = 192\n",
      "found 100 tweets\n",
      "count = 193\n",
      "found 100 tweets\n",
      "count = 194\n",
      "found 100 tweets\n",
      "count = 195\n",
      "found 100 tweets\n",
      "count = 196\n",
      "found 100 tweets\n",
      "count = 197\n",
      "found 100 tweets\n",
      "count = 198\n",
      "found 100 tweets\n",
      "count = 199\n",
      "found 100 tweets\n",
      "count = 200\n",
      "found 100 tweets\n",
      "count = 201\n",
      "found 100 tweets\n",
      "count = 202\n",
      "found 100 tweets\n",
      "count = 203\n",
      "found 100 tweets\n",
      "count = 204\n",
      "found 100 tweets\n",
      "count = 205\n",
      "found 100 tweets\n",
      "count = 206\n",
      "found 100 tweets\n",
      "count = 207\n",
      "found 100 tweets\n",
      "count = 208\n",
      "found 100 tweets\n",
      "count = 209\n",
      "found 100 tweets\n",
      "count = 210\n",
      "found 100 tweets\n",
      "count = 211\n",
      "found 100 tweets\n",
      "count = 212\n",
      "found 100 tweets\n",
      "count = 213\n",
      "found 100 tweets\n",
      "count = 214\n",
      "found 100 tweets\n",
      "count = 215\n",
      "found 100 tweets\n",
      "count = 216\n",
      "found 100 tweets\n",
      "count = 217\n",
      "found 100 tweets\n",
      "count = 218\n",
      "found 100 tweets\n",
      "count = 219\n",
      "found 100 tweets\n",
      "count = 220\n",
      "found 100 tweets\n",
      "count = 221\n",
      "found 100 tweets\n",
      "count = 222\n",
      "found 100 tweets\n",
      "count = 223\n",
      "found 100 tweets\n",
      "count = 224\n",
      "found 100 tweets\n",
      "count = 225\n",
      "found 100 tweets\n",
      "count = 226\n",
      "found 100 tweets\n",
      "count = 227\n",
      "found 100 tweets\n",
      "count = 228\n",
      "found 100 tweets\n",
      "count = 229\n",
      "found 100 tweets\n",
      "count = 230\n",
      "found 100 tweets\n",
      "count = 231\n",
      "found 100 tweets\n",
      "count = 232\n",
      "found 100 tweets\n",
      "count = 233\n",
      "found 100 tweets\n",
      "count = 234\n",
      "found 100 tweets\n",
      "count = 235\n",
      "found 100 tweets\n",
      "count = 236\n",
      "found 100 tweets\n",
      "count = 237\n",
      "found 100 tweets\n",
      "count = 238\n",
      "found 100 tweets\n",
      "count = 239\n",
      "found 100 tweets\n",
      "count = 240\n",
      "found 100 tweets\n",
      "count = 241\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 242\n",
      "found 100 tweets\n",
      "count = 243\n",
      "found 100 tweets\n",
      "count = 244\n",
      "found 100 tweets\n",
      "count = 245\n",
      "found 100 tweets\n",
      "count = 246\n",
      "found 100 tweets\n",
      "count = 247\n",
      "found 100 tweets\n",
      "count = 248\n",
      "found 100 tweets\n",
      "count = 249\n",
      "found 100 tweets\n",
      "count = 250\n",
      "found 100 tweets\n",
      "count = 251\n",
      "found 100 tweets\n",
      "count = 252\n",
      "found 100 tweets\n",
      "count = 253\n",
      "found 100 tweets\n",
      "count = 254\n",
      "found 100 tweets\n",
      "count = 255\n",
      "found 100 tweets\n",
      "count = 256\n",
      "found 100 tweets\n",
      "count = 257\n",
      "found 100 tweets\n",
      "count = 258\n",
      "found 100 tweets\n",
      "count = 259\n",
      "found 100 tweets\n",
      "count = 260\n",
      "found 100 tweets\n",
      "count = 261\n",
      "found 100 tweets\n",
      "count = 262\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 263\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 264\n",
      "found 100 tweets\n",
      "count = 265\n",
      "found 100 tweets\n",
      "count = 266\n",
      "found 100 tweets\n",
      "count = 267\n",
      "found 100 tweets\n",
      "count = 268\n",
      "found 100 tweets\n",
      "count = 269\n",
      "found 100 tweets\n",
      "count = 270\n",
      "found 100 tweets\n",
      "count = 271\n",
      "found 100 tweets\n",
      "count = 272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 tweets\n",
      "count = 273\n",
      "found 100 tweets\n",
      "count = 274\n",
      "found 100 tweets\n",
      "count = 275\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 276\n",
      "found 100 tweets\n",
      "count = 277\n",
      "found 100 tweets\n",
      "count = 278\n",
      "found 100 tweets\n",
      "count = 279\n",
      "found 100 tweets\n",
      "count = 280\n",
      "found 100 tweets\n",
      "count = 281\n",
      "found 100 tweets\n",
      "count = 282\n",
      "found 100 tweets\n",
      "count = 283\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 284\n",
      "found 100 tweets\n",
      "count = 285\n",
      "found 100 tweets\n",
      "count = 286\n",
      "found 100 tweets\n",
      "count = 287\n",
      "found 100 tweets\n",
      "count = 288\n",
      "found 100 tweets\n",
      "count = 289\n",
      "found 100 tweets\n",
      "count = 290\n",
      "found 100 tweets\n",
      "count = 291\n",
      "found 100 tweets\n",
      "count = 292\n",
      "found 100 tweets\n",
      "count = 293\n",
      "found 100 tweets\n",
      "count = 294\n",
      "found 100 tweets\n",
      "count = 295\n",
      "found 100 tweets\n",
      "count = 296\n",
      "found 100 tweets\n",
      "count = 297\n",
      "found 100 tweets\n",
      "count = 298\n",
      "found 100 tweets\n",
      "count = 299\n",
      "found 100 tweets\n",
      "count = 300\n",
      "found 100 tweets\n",
      "count = 301\n",
      "found 100 tweets\n",
      "count = 302\n",
      "found 100 tweets\n",
      "count = 303\n",
      "found 100 tweets\n",
      "count = 304\n",
      "found 100 tweets\n",
      "count = 305\n",
      "found 100 tweets\n",
      "count = 306\n",
      "found 100 tweets\n",
      "count = 307\n",
      "found 100 tweets\n",
      "count = 308\n",
      "found 100 tweets\n",
      "count = 309\n",
      "found 100 tweets\n",
      "count = 310\n",
      "found 100 tweets\n",
      "count = 311\n",
      "found 100 tweets\n",
      "count = 312\n",
      "found 100 tweets\n",
      "count = 313\n",
      "found 100 tweets\n",
      "count = 314\n",
      "found 100 tweets\n",
      "count = 315\n",
      "found 100 tweets\n",
      "count = 316\n",
      "found 100 tweets\n",
      "count = 317\n",
      "found 100 tweets\n",
      "count = 318\n",
      "found 100 tweets\n",
      "count = 319\n",
      "found 100 tweets\n",
      "count = 320\n",
      "found 100 tweets\n",
      "count = 321\n",
      "found 100 tweets\n",
      "count = 322\n",
      "found 100 tweets\n",
      "count = 323\n",
      "found 100 tweets\n",
      "count = 324\n",
      "found 100 tweets\n",
      "count = 325\n",
      "found 100 tweets\n",
      "count = 326\n",
      "found 100 tweets\n",
      "count = 327\n",
      "found 100 tweets\n",
      "count = 328\n",
      "found 100 tweets\n",
      "count = 329\n",
      "found 100 tweets\n",
      "count = 330\n",
      "found 100 tweets\n",
      "count = 331\n",
      "found 100 tweets\n",
      "count = 332\n",
      "found 100 tweets\n",
      "count = 333\n",
      "found 100 tweets\n",
      "count = 334\n",
      "found 100 tweets\n",
      "count = 335\n",
      "found 100 tweets\n",
      "count = 336\n",
      "found 100 tweets\n",
      "count = 337\n",
      "found 100 tweets\n",
      "count = 338\n",
      "found 100 tweets\n",
      "count = 339\n",
      "found 100 tweets\n",
      "count = 340\n",
      "found 100 tweets\n",
      "count = 341\n",
      "found 100 tweets\n",
      "count = 342\n",
      "found 100 tweets\n",
      "count = 343\n",
      "found 100 tweets\n",
      "count = 344\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 345\n",
      "exception raised, waiting 15 minutes\n",
      "(until: 2021-11-11 20:38:16.191016 )\n",
      "count = 346\n",
      "found 100 tweets\n",
      "count = 347\n",
      "found 100 tweets\n",
      "count = 348\n",
      "found 100 tweets\n",
      "count = 349\n",
      "found 100 tweets\n",
      "count = 350\n",
      "found 100 tweets\n",
      "count = 351\n",
      "found 100 tweets\n",
      "count = 352\n",
      "found 100 tweets\n",
      "count = 353\n",
      "found 100 tweets\n",
      "count = 354\n",
      "found 100 tweets\n",
      "count = 355\n",
      "found 100 tweets\n",
      "count = 356\n",
      "found 100 tweets\n",
      "count = 357\n",
      "found 100 tweets\n",
      "count = 358\n",
      "found 100 tweets\n",
      "count = 359\n",
      "found 100 tweets\n",
      "count = 360\n",
      "found 100 tweets\n",
      "count = 361\n",
      "found 100 tweets\n",
      "count = 362\n",
      "found 100 tweets\n",
      "count = 363\n",
      "found 100 tweets\n",
      "count = 364\n",
      "found 100 tweets\n",
      "count = 365\n",
      "found 100 tweets\n",
      "count = 366\n",
      "found 100 tweets\n",
      "count = 367\n",
      "found 100 tweets\n",
      "count = 368\n",
      "found 100 tweets\n",
      "count = 369\n",
      "found 100 tweets\n",
      "count = 370\n",
      "found 100 tweets\n",
      "count = 371\n",
      "found 99 tweets\n",
      "found 0 tweets\n",
      "no tweets found\n",
      "count = 372\n",
      "found 100 tweets\n",
      "count = 373\n",
      "found 100 tweets\n",
      "count = 374\n",
      "found 100 tweets\n",
      "count = 375\n",
      "found 100 tweets\n",
      "count = 376\n",
      "found 100 tweets\n",
      "count = 377\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 378\n",
      "found 100 tweets\n",
      "count = 379\n",
      "found 100 tweets\n",
      "count = 380\n",
      "found 100 tweets\n",
      "count = 381\n",
      "found 100 tweets\n",
      "count = 382\n",
      "found 100 tweets\n",
      "count = 383\n",
      "found 99 tweets\n",
      "found 1 tweets\n",
      "count = 384\n",
      "found 100 tweets\n",
      "count = 385\n",
      "found 100 tweets\n",
      "count = 386\n",
      "found 100 tweets\n",
      "count = 387\n",
      "found 100 tweets\n",
      "count = 388\n",
      "found 100 tweets\n",
      "count = 389\n",
      "found 100 tweets\n",
      "count = 390\n",
      "found 100 tweets\n",
      "count = 391\n",
      "found 100 tweets\n",
      "count = 392\n",
      "found 100 tweets\n",
      "count = 393\n",
      "found 100 tweets\n",
      "count = 394\n",
      "found 100 tweets\n",
      "count = 395\n",
      "found 100 tweets\n",
      "count = 396\n",
      "found 100 tweets\n",
      "count = 397\n",
      "found 25 tweets\n",
      "found 0 tweets\n",
      "no tweets found\n",
      "count = 398\n",
      "found 0 tweets\n",
      "no tweets found\n",
      "count = 399\n",
      "found 0 tweets\n",
      "no tweets found\n",
      "count = 400\n",
      "found 0 tweets\n",
      "no tweets found\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Maximum number of empty tweet strings reached - exiting",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Maximum number of empty tweet strings reached - exiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akskulka/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "'''\n",
    "In order to use this script you should register a data-mining application\n",
    "with Twitter.  Good instructions for doing so can be found here:\n",
    "http://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/\n",
    "After doing this you can copy and paste your unique consumer key,\n",
    "consumer secret, access token, and access secret into the load_api()\n",
    "function below.\n",
    "The main() function can be run by executing the command: \n",
    "python twitter_search.py\n",
    "I used Python 3 and tweepy version 3.5.0.  You will also need the other\n",
    "packages imported above.\n",
    "'''\n",
    "\n",
    "def load_api():\n",
    "    ''' Function that loads the twitter API after authorizing the user. '''\n",
    "\n",
    "    consumer_key = 'c68ZDMTYHivwc4RETaj55ETxa'\n",
    "    consumer_secret = 'Ng3qsFjwPzCjS69VKsN3VcgwVS3fpyej1zhgCdorkC2uMWZLVo'\n",
    "    access_token = '1003856940642910209-gT4vaK9m1ReYbc5hW7tPG7qYY27AyW'\n",
    "    access_token_secret = 'O60Kd73sg7CHyh3N8efzZvXthem5VpGlm0Sb79xSC54ym'\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    # load the twitter API via tweepy\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "    \n",
    "def tweet_search(api, query, max_tweets, max_id, since_id, geocode):\n",
    "    ''' Function that takes in a search string 'query', the maximum\n",
    "        number of tweets 'max_tweets', and the minimum (i.e., starting)\n",
    "        tweet id. It returns a list of tweepy.models.Status objects. '''\n",
    "\n",
    "    searched_tweets = []\n",
    "    while len(searched_tweets) < max_tweets:\n",
    "        remaining_tweets = max_tweets - len(searched_tweets)\n",
    "        try:\n",
    "            new_tweets = api.search(q=query, count=remaining_tweets,\n",
    "                                    since_id=str(since_id), max_id=str(max_id-1))\n",
    "#                                    geocode=geocode)\n",
    "            print('found',len(new_tweets),'tweets')\n",
    "            if not new_tweets:\n",
    "                print('no tweets found')\n",
    "                break\n",
    "            searched_tweets.extend(new_tweets)\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError:\n",
    "            print('exception raised, waiting 15 minutes')\n",
    "            print('(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')')\n",
    "            time.sleep(15*60)\n",
    "            break # stop the loop\n",
    "    return searched_tweets, max_id\n",
    "\n",
    "\n",
    "def get_tweet_id(api, date='', days_ago=9, query='a'):\n",
    "    ''' Function that gets the ID of a tweet. This ID can then be\n",
    "        used as a 'starting point' from which to search. The query is\n",
    "        required and has been set to a commonly used word by default.\n",
    "        The variable 'days_ago' has been initialized to the maximum\n",
    "        amount we are able to search back in time (9).'''\n",
    "\n",
    "    if date:\n",
    "        # return an ID from the start of the given day\n",
    "        td = date + dt.timedelta(days=1)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        tweet = api.search(q=query, count=1, until=tweet_date)\n",
    "    else:\n",
    "        # return an ID from __ days ago\n",
    "        td = dt.datetime.now() - dt.timedelta(days=days_ago)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        # get list of up to 10 tweets\n",
    "        tweet = api.search(q=query, count=10, until=tweet_date)\n",
    "        print('search limit (start/stop):',tweet[0].created_at)\n",
    "        # return the id of the first tweet in the list\n",
    "        return tweet[0].id\n",
    "\n",
    "\n",
    "def write_tweets(tweets, filename):\n",
    "    ''' Function that appends tweets to a file. '''\n",
    "\n",
    "    with open(filename, 'a') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet._json, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def main():\n",
    "    ''' This is a script that continuously searches for tweets\n",
    "        that were created over a given number of days. The search\n",
    "        dates and search phrase can be changed below. '''\n",
    "\n",
    "\n",
    "\n",
    "    ''' search variables: '''\n",
    "    search_phrases = ['samsung']\n",
    "    \n",
    "    \n",
    "    \n",
    "    time_limit = 1.5                           # runtime limit in hours\n",
    "    max_tweets = 100                           # number of tweets per search (will be\n",
    "                                               # iterated over) - maximum is 100\n",
    "    min_days_old, max_days_old = 6, 7          # search limits e.g., from 7 to 8\n",
    "                                               # gives current weekday from last week,\n",
    "                                               # min_days_old=0 will search from right now\n",
    "    USA = '39.8,-95.583068847656,2500km'       # this geocode includes nearly all American\n",
    "                                               # states (and a large portion of Canada)\n",
    "    \n",
    "\n",
    "    # loop over search items,\n",
    "    # creating a new file for each\n",
    "    for search_phrase in search_phrases:\n",
    "\n",
    "        print('Search phrase =', search_phrase)\n",
    "\n",
    "        ''' other variables '''\n",
    "        name = search_phrase.split()[0]\n",
    "        json_file_root = name + '/'  + name\n",
    "        os.makedirs(os.path.dirname(json_file_root), exist_ok=True)\n",
    "        read_IDs = False\n",
    "        \n",
    "        # open a file in which to store the tweets\n",
    "        if max_days_old - min_days_old == 1:\n",
    "            d = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}'.format(d.year, d.month, d.day)\n",
    "        else:\n",
    "            d1 = dt.datetime.now() - dt.timedelta(days=max_days_old-1)\n",
    "            d2 = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}_to_{3}-{4:0>2}-{5:0>2}'.format(\n",
    "                  d1.year, d1.month, d1.day, d2.year, d2.month, d2.day)\n",
    "        json_file = json_file_root + '_' + day + '.json'\n",
    "        if os.path.isfile(json_file):\n",
    "            print('Appending tweets to file named: ',json_file)\n",
    "            read_IDs = True\n",
    "        \n",
    "        # authorize and load the twitter API\n",
    "        api = load_api()\n",
    "        \n",
    "        # set the 'starting point' ID for tweet collection\n",
    "        if read_IDs:\n",
    "            # open the json file and get the latest tweet ID\n",
    "            with open(json_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                max_id = json.loads(lines[-1])['id']\n",
    "                print('Searching from the bottom ID in file')\n",
    "        else:\n",
    "            # get the ID of a tweet that is min_days_old\n",
    "            if min_days_old == 0:\n",
    "                max_id = -1\n",
    "            else:\n",
    "                max_id = get_tweet_id(api, days_ago=(min_days_old-1))\n",
    "        # set the smallest ID to search for\n",
    "        since_id = get_tweet_id(api, days_ago=(max_days_old-1))\n",
    "        print('max id (starting point) =', max_id)\n",
    "        print('since id (ending point) =', since_id)\n",
    "        \n",
    "\n",
    "\n",
    "        ''' tweet gathering loop  '''\n",
    "        start = dt.datetime.now()\n",
    "        end = start + dt.timedelta(hours=time_limit)\n",
    "        count, exitcount = 0, 0\n",
    "        while dt.datetime.now() < end:\n",
    "            count += 1\n",
    "            print('count =',count)\n",
    "            # collect tweets and update max_id\n",
    "            tweets, max_id = tweet_search(api, search_phrase, max_tweets,\n",
    "                                          max_id=max_id, since_id=since_id,\n",
    "                                          geocode=USA)\n",
    "            \n",
    "            # write tweets to file in JSON format\n",
    "            if tweets:\n",
    "                write_tweets(tweets, json_file)\n",
    "                exitcount = 0\n",
    "            else:\n",
    "                exitcount += 1\n",
    "                if exitcount == 3:\n",
    "                    if search_phrase == search_phrases[-1]:\n",
    "                        sys.exit('Maximum number of empty tweet strings reached - exiting')\n",
    "                    else:\n",
    "                        print('Maximum number of empty tweet strings reached - breaking')\n",
    "                        break\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ce15c",
   "metadata": {},
   "source": [
    "# Reading the extracted twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd72933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "twt=pd.read_json('samsung/samsung_2021-11-05.json',lines=True,orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b2255",
   "metadata": {},
   "source": [
    "# Selecting the required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99226b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "twt = twt[[ 'created_at','text']] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d35410",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53af2a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akskulka/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    " #text preprocess for twitter data\n",
    "twt['text'] = twt['text'].str.lstrip('0123456789')\n",
    "#lower casing\n",
    "twt['text'] = twt['text'].apply(lambda a: \" \".join(a.lower() for a in a.split()))\n",
    "#remove punctuation\n",
    "twt['text'] = twt['text'].str.replace('[^\\w\\s]','')\n",
    "#remove stopwords\n",
    "sw = stopwords.words('english')\n",
    "twt['text'] = twt['text'].apply(lambda a: \" \".join(a for a in a.split() if a not in sw))\n",
    "#spelling correction\n",
    "#twt['text'].apply(lambda a: str(TextBlob(a).correct()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1b5797b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39418</th>\n",
       "      <td>2021-11-05 00:00:04+00:00</td>\n",
       "      <td>kabum tablet samsung galaxy a7 lite 4g 32gb an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39419</th>\n",
       "      <td>2021-11-05 00:00:03+00:00</td>\n",
       "      <td>rt kirstingives 100 4600 1400000 idr samsung g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39420</th>\n",
       "      <td>2021-11-05 00:00:02+00:00</td>\n",
       "      <td>tanyarl halou samsung user mau nanya dong ini ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39421</th>\n",
       "      <td>2021-11-05 00:00:01+00:00</td>\n",
       "      <td>samsung quickstar samsung good lock se actuali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39422</th>\n",
       "      <td>2021-11-05 00:00:00+00:00</td>\n",
       "      <td>to5toys last call amazon halo band 75 hisense ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     created_at  \\\n",
       "39418 2021-11-05 00:00:04+00:00   \n",
       "39419 2021-11-05 00:00:03+00:00   \n",
       "39420 2021-11-05 00:00:02+00:00   \n",
       "39421 2021-11-05 00:00:01+00:00   \n",
       "39422 2021-11-05 00:00:00+00:00   \n",
       "\n",
       "                                                    text  \n",
       "39418  kabum tablet samsung galaxy a7 lite 4g 32gb an...  \n",
       "39419  rt kirstingives 100 4600 1400000 idr samsung g...  \n",
       "39420  tanyarl halou samsung user mau nanya dong ini ...  \n",
       "39421  samsung quickstar samsung good lock se actuali...  \n",
       "39422  to5toys last call amazon halo band 75 hisense ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt.tail()###########df-twt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f178733",
   "metadata": {},
   "source": [
    "# Convert to features and predict the emotions using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f499a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39418</th>\n",
       "      <td>2021-11-05 00:00:04+00:00</td>\n",
       "      <td>kabum tablet samsung galaxy a7 lite 4g 32gb an...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39419</th>\n",
       "      <td>2021-11-05 00:00:03+00:00</td>\n",
       "      <td>rt kirstingives 100 4600 1400000 idr samsung g...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39420</th>\n",
       "      <td>2021-11-05 00:00:02+00:00</td>\n",
       "      <td>tanyarl halou samsung user mau nanya dong ini ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39421</th>\n",
       "      <td>2021-11-05 00:00:01+00:00</td>\n",
       "      <td>samsung quickstar samsung good lock se actuali...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39422</th>\n",
       "      <td>2021-11-05 00:00:00+00:00</td>\n",
       "      <td>to5toys last call amazon halo band 75 hisense ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     created_at  \\\n",
       "39418 2021-11-05 00:00:04+00:00   \n",
       "39419 2021-11-05 00:00:03+00:00   \n",
       "39420 2021-11-05 00:00:02+00:00   \n",
       "39421 2021-11-05 00:00:01+00:00   \n",
       "39422 2021-11-05 00:00:00+00:00   \n",
       "\n",
       "                                                    text  Emotion  \n",
       "39418  kabum tablet samsung galaxy a7 lite 4g 32gb an...        6  \n",
       "39419  rt kirstingives 100 4600 1400000 idr samsung g...        2  \n",
       "39420  tanyarl halou samsung user mau nanya dong ini ...        1  \n",
       "39421  samsung quickstar samsung good lock se actuali...        4  \n",
       "39422  to5toys last call amazon halo band 75 hisense ...        0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xpredict =  twt['text']\n",
    "    \n",
    "# word level tf-idf\n",
    "predict_tfidf =  tv.transform(Xpredict)#########tfidf-tvv\n",
    "    \n",
    "# Get  Predicted Emotion\n",
    "twt['Emotion'] = classifier.predict(predict_tfidf)\n",
    "\n",
    "twt.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f589b",
   "metadata": {},
   "source": [
    "# Predicting the sentiment using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f17a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "twt['sentiment'] = twt['text'].apply(lambda a: TextBlob(a).sentiment[0] )\n",
    "    \n",
    "        \n",
    "def function (value):\n",
    "    if value['sentiment'] < 0:#######indeennt\n",
    "        return 'Negative'\n",
    "    if value['sentiment'] > 0:\n",
    "        return 'Positive'\n",
    "    return 'Neutral'\n",
    "twt['Sentiment_label'] = twt.apply (lambda a: function(a),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9817c94",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c118b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39418</th>\n",
       "      <td>2021-11-05 00:00:04+00:00</td>\n",
       "      <td>kabum tablet samsung galaxy a7 lite 4g 32gb an...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39419</th>\n",
       "      <td>2021-11-05 00:00:03+00:00</td>\n",
       "      <td>rt kirstingives 100 4600 1400000 idr samsung g...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39420</th>\n",
       "      <td>2021-11-05 00:00:02+00:00</td>\n",
       "      <td>tanyarl halou samsung user mau nanya dong ini ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39421</th>\n",
       "      <td>2021-11-05 00:00:01+00:00</td>\n",
       "      <td>samsung quickstar samsung good lock se actuali...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39422</th>\n",
       "      <td>2021-11-05 00:00:00+00:00</td>\n",
       "      <td>to5toys last call amazon halo band 75 hisense ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     created_at  \\\n",
       "39418 2021-11-05 00:00:04+00:00   \n",
       "39419 2021-11-05 00:00:03+00:00   \n",
       "39420 2021-11-05 00:00:02+00:00   \n",
       "39421 2021-11-05 00:00:01+00:00   \n",
       "39422 2021-11-05 00:00:00+00:00   \n",
       "\n",
       "                                                    text  Emotion  sentiment  \\\n",
       "39418  kabum tablet samsung galaxy a7 lite 4g 32gb an...        6        0.0   \n",
       "39419  rt kirstingives 100 4600 1400000 idr samsung g...        2        0.0   \n",
       "39420  tanyarl halou samsung user mau nanya dong ini ...        1        0.0   \n",
       "39421  samsung quickstar samsung good lock se actuali...        4        0.7   \n",
       "39422  to5toys last call amazon halo band 75 hisense ...        0        0.0   \n",
       "\n",
       "      Sentiment_label  \n",
       "39418         Neutral  \n",
       "39419         Neutral  \n",
       "39420         Neutral  \n",
       "39421        Positive  \n",
       "39422         Neutral  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f6ac3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akskulka/anaconda3/lib/python3.7/site-packages/plotly/offline/offline.py:563: UserWarning:\n",
      "\n",
      "Your filename `Sentiment` didn't end with .html. Adding .html to the end of your file.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sentiment.html'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly as ply\n",
    "import cufflinks as cf\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import *\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "cf.set_config_file(offline=True, world_readable=True, theme='white')\n",
    "\n",
    "Sentiment_df = pd.DataFrame(twt.Sentiment_label.value_counts().reset_index())#####df-twt\n",
    "Sentiment_df.columns = ['Sentiment', 'Count']\n",
    "    \n",
    "Sentiment_df = pd.DataFrame(Sentiment_df)\n",
    "\n",
    "Sentiment_df['Percentage'] = 100 * Sentiment_df['Count']/ Sentiment_df['Count'].sum()\n",
    "    \n",
    "Sentiment_Max = Sentiment_df.iloc[0,0]\n",
    "Sentiment_percent = str(round(Sentiment_df.iloc[0,2],2))\n",
    "    \n",
    "    \n",
    "fig1 = Sentiment_df.iplot(kind='pie',labels='Sentiment',values='Count',textinfo='label+percent', title= 'Sentiment Analysis', world_readable=True,\n",
    "                    asFigure=True)\n",
    "        \n",
    "ply.offline.plot(fig1,filename=\"Sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a84cc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akskulka/anaconda3/lib/python3.7/site-packages/plotly/offline/offline.py:563: UserWarning:\n",
      "\n",
      "Your filename `Emotion` didn't end with .html. Adding .html to the end of your file.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Emotion.html'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly as ply\n",
    "import cufflinks as cf\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import *\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "cf.set_config_file(offline=True, world_readable=True, theme='white')\n",
    "Emotion_df = pd.DataFrame(twt.Emotion.value_counts().reset_index())####df-twt\n",
    "Emotion_df.columns = ['Emotion', 'Count']\n",
    "Emotion_df = pd.DataFrame (Emotion_df)\n",
    "Emotion_df['Percentage'] = 100 * Emotion_df['Count']/ Emotion_df['Count'].sum()\n",
    "\n",
    "Emotion_Max = Emotion_df.iloc[0,0]\n",
    "Emotion_percent = str(round(Emotion_df.iloc[0,2],2))\n",
    "fig = Emotion_df.iplot(kind='pie', labels = 'Emotion', values = 'Count',pull= .2, hole=.2 , colorscale = 'reds', textposition='outside',colors=['red','green','purple','orange','blue','yellow','pink'],textinfo='label+percent', title= 'Emotion Analysis', world_readable=True,asFigure=True)\n",
    "ply.offline.plot(fig,filename=\"Emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c811f4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "    \n",
    "Result = pd.crosstab(twt.Emotion, twt.Sentiment_label)\n",
    "plt = Result.plot.bar(stacked=True,sort_columns = True)\n",
    "plt.legend(title='Sentiment_label')\n",
    "plt.figure.savefig('Emotion_Sentiment_stacked.png', dpi=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042ee3f",
   "metadata": {},
   "source": [
    "# Email generation code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import os\n",
    "\n",
    "from email.mime.application import MIMEApplication\n",
    "from email import encoders\n",
    "import smtplib\n",
    "\n",
    "\n",
    "def generate_email():\n",
    "    \n",
    "    dir_path = \"Add PATH\"\n",
    "    files = [\"Add files\"]\n",
    "    \n",
    "    # Add concerned address (you can add multiple address also) and Password\n",
    "    company_dict = ['xyz@gmail.com']\n",
    "    password = \"Password\"\n",
    "    \n",
    "    for value in company_dict:\n",
    "        # Add From email address\n",
    "        From_address = 'From email id'\n",
    "        To_address = value\n",
    "\n",
    "        text = MIMEMultipart()\n",
    "\n",
    "        text['From'] = \"xxxx\"\n",
    "        text['To'] = To_address\n",
    "        text['Subject'] = \"Emotion Detection and Sentiment Analysis Report\"\n",
    "\n",
    "        body = \" Hai \\n Greetings of the day,\\n We would like to inform you that the data is more about, \\n Emotion -  \"+Emotion_Max+\" (\"+Emotion_percent+\" %).\\n Sentiment - \" +Sentiment_Max+\" (\"+Sentiment_percent+\" %).\\n\\n For the details please go through the attachments bellow. \\n\\n\\n\\n\\n Thank You.\"\n",
    "\n",
    "        text.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "\n",
    "        for k in files:  # add files to the message\n",
    "        \n",
    "            file_location = os.path.join(dir_location, k)\n",
    "            attachment = MIMEApplication(open(file_location, \"rb\").read(), _subtype=\"txt\")\n",
    "            attachment.add_header('Content-Disposition',obj, filename=k)\n",
    "            text.attach(attachment)\n",
    "\n",
    "        smtp = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "        smtp.login(From_address, password)\n",
    "        text1 = text.as_string()\n",
    "        smtp.sendmail(From_address, To_address, text1)\n",
    "        smtp.quit()\n",
    "        \n",
    "        return 'Successfully Sent..!'\n",
    "\n",
    "\n",
    "#Calling the function to send reports\n",
    "          \n",
    "generate_email ()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
