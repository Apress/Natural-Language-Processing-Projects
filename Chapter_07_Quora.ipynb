{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 07: Quora.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "4SiSNzMdkTsP",
        "outputId": "9b342c18-4303-4401-885d-c82425d46bac"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import scipy\n",
        "import string\n",
        "import csv\n",
        "\n",
        "#import nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#immport tokenize, stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "#import warnings\n",
        "import warnings\n",
        "\n",
        "#import sklearn and matplotlib\n",
        "from sklearn import preprocessing\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt \n",
        "import plotly.graph_objects as go\n",
        "\n",
        "#import warning\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n",
        "\n",
        "\n",
        "#import the data\n",
        "train=pd.read_csv('Quora.csv')\n",
        "train1=train.copy()\n",
        "\n",
        "train.head()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  ...                                          question2 is_duplicate\n",
              "0   0     1  ...  What is the step by step guide to invest in sh...            0\n",
              "1   1     3  ...  What would happen if the Indian government sto...            0\n",
              "2   2     5  ...  How can Internet speed be increased by hacking...            0\n",
              "3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n",
              "4   4     9  ...            Which fish would survive in salt water?            0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PoC3TaexkyT4",
        "outputId": "fbe2026a-ea87-4d05-d956-444f1ce7515b"
      },
      "source": [
        "#append the both set of questions in dataset\n",
        "Q1=train1.iloc[:,[2,4]]\n",
        "Q2=train1.iloc[:,[1,3]]\n",
        "\n",
        "df = pd.DataFrame( np.concatenate( (Q2.values, Q1.values), axis=0 ) )\n",
        "df.columns = ['id','question' ]\n",
        "df\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9851</td>\n",
              "      <td>How does a pirate radio station work?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9853</td>\n",
              "      <td>Which mobile is good within 20k?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9855</td>\n",
              "      <td>What actually happened in predestination?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9857</td>\n",
              "      <td>Are Near Death Experiences (NDEs) real?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9859</td>\n",
              "      <td>Can you be rich with $500,000?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                           question\n",
              "0        1  What is the step by step guide to invest in sh...\n",
              "1        3  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
              "2        5  How can I increase the speed of my internet co...\n",
              "3        7  Why am I mentally very lonely? How can I solve...\n",
              "4        9  Which one dissolve in water quikly sugar, salt...\n",
              "...    ...                                                ...\n",
              "9995  9851              How does a pirate radio station work?\n",
              "9996  9853                   Which mobile is good within 20k?\n",
              "9997  9855          What actually happened in predestination?\n",
              "9998  9857            Are Near Death Experiences (NDEs) real?\n",
              "9999  9859                     Can you be rich with $500,000?\n",
              "\n",
              "[10000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofU11-d5k7WE"
      },
      "source": [
        "# A.\t Building vectors using **Doc2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReAkCbDukyXT"
      },
      "source": [
        "# importing doc2vec from gensim \n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument \n",
        "\n",
        "# tokenizing the sentences\n",
        "tok_quora=[word_tokenize(wrd) for wrd in df.question]\n",
        "\n",
        "#creating training data\n",
        "Quora_training_data=[TaggedDocument(d, [i]) for i, d in enumerate(tok_quora)]   \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5B7CRv9kyat"
      },
      "source": [
        "# trainin doc2vec model\n",
        "doc_model = Doc2Vec(Quora_training_data, vector_size = 100, window = 5, min_count = 3, epochs = 25)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5gGV1AWkydD"
      },
      "source": [
        "#function to get vectors from model\n",
        "\n",
        "def fetch_embeddings(model,tokens):\n",
        "  tokens = [x for x in word_tokenize(tokens) if x in list(doc_model.wv.vocab)]\n",
        "  #if words is not present then vector becomes zero\n",
        "  if len(tokens)>=1:\n",
        "    return doc_model.infer_vector(tokens)\n",
        "  else:\n",
        "    return np.array([0]*100)  \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFP5-S-CkyhA"
      },
      "source": [
        "#Storing all embedded sentence vectors in a list \n",
        "\n",
        "#defining empty list and iterating through all the questions\n",
        "\n",
        "doc_embeddings=[]                                     \n",
        "for w in df.question:\n",
        "    doc_embeddings.append(list(fetch_embeddings(doc_model, w)))  \n",
        "#conveting it into array\n",
        "doc_embeddings=np.asarray(doc_embeddings)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io2reG9wlOW_"
      },
      "source": [
        "# B.\t Sentence Transformers using BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDwqf5DVkykZ",
        "outputId": "84a59499-ba80-4e8f-efa9-c55fae7ffa92"
      },
      "source": [
        "#install SBERT\n",
        "!pip install sentence-transformers\n",
        "\n",
        "#import the SBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#let use paraphrase-MiniLM-L12-v2 pre trained model\n",
        "sbert_model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
        "\n",
        "x=[i for i in df.question]\n",
        "#lets get embeddings for each question\n",
        "sentence_embeddings_BERT= sbert_model.encode(x)\n",
        "\n",
        "#lets see the shape\n",
        "sentence_embeddings_BERT.shape\n",
        "\n",
        "sentence_embeddings_BERT"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.17)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.10.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.15299696, -0.30485195,  0.00183832, ..., -0.27034327,\n",
              "        -0.4260835 ,  0.31928647],\n",
              "       [-0.16776392,  0.67119426, -0.51778895, ..., -0.08420195,\n",
              "         0.00470432,  0.3200466 ],\n",
              "       [ 0.09468909, -0.00629827,  0.06894321, ..., -0.18716985,\n",
              "        -0.2556718 ,  0.04188535],\n",
              "       ...,\n",
              "       [-0.03576591,  0.24553823,  0.10434522, ...,  0.1781153 ,\n",
              "        -0.00719658, -0.19804102],\n",
              "       [ 0.15157805, -0.32189795,  0.01304498, ..., -0.05349696,\n",
              "        -0.33300248,  0.18506213],\n",
              "       [ 0.2177582 , -0.07103815,  0.11279581, ..., -0.5100467 ,\n",
              "         0.05511575, -0.1291719 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Btc0xCYlZKS"
      },
      "source": [
        "# C.\tGPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YCfa0ZwCkynU",
        "outputId": "86e21484-3d91-48e9-bedd-3f42ece5adb3"
      },
      "source": [
        "#Installing the GPT\n",
        "!pip install pytorch_pretrained_bert \n",
        "\n",
        "#Importing required tokenizer, OpenAiGPT model\n",
        "import torch\n",
        "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel\n",
        "\n",
        "#initializing the tokenizer\n",
        "tok_gpt= OpenAIGPTTokenizer.from_pretrained('openai-gpt')  \n",
        "\n",
        "#Initializing the gpt Model\n",
        "model_gpt= OpenAIGPTModel.from_pretrained('openai-gpt')\n",
        "model_gpt.eval()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.62.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.9.0+cu102)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.18.44-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.44\n",
            "  Downloading botocore-1.21.44-py3-none-any.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 52.7 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.44->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 68.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.44->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.5.30)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.18.44 botocore-1.21.44 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.0 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 815973/815973 [00:00<00:00, 2197331.83B/s]\n",
            "100%|██████████| 458495/458495 [00:00<00:00, 1501209.52B/s]\n",
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
            "100%|██████████| 478750579/478750579 [00:13<00:00, 35642100.42B/s]\n",
            "100%|██████████| 656/656 [00:00<00:00, 281278.21B/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIGPTModel(\n",
              "  (tokens_embed): Embedding(40478, 768)\n",
              "  (positions_embed): Embedding(512, 768)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              "  (h): ModuleList(\n",
              "    (0): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (4): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (7): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (9): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (10): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "    (11): Block(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): BertLayerNorm()\n",
              "      (mlp): MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): BertLayerNorm()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOnO7TKKqEF9"
      },
      "source": [
        "def Fetch_gpt_vectors(question):\n",
        "  \n",
        "  #tokenize words\n",
        "  words = word_tokenize(question)\n",
        "  emb = np.zeros((1,768))\n",
        "\n",
        "  #get vectore for each word\n",
        "  for word in words:\n",
        "      w= tok_gpt.tokenize(word)\n",
        "      indexed_words = tok_gpt.convert_tokens_to_ids(w)\n",
        "      tns_word = torch.tensor([indexed_words])\n",
        "\n",
        "      with torch.no_grad():\n",
        "          try:\n",
        "     #get mean vector\n",
        "            emb += np.array(torch.mean(model_gpt(tns_word),1))\n",
        "          except Exception as e:\n",
        "            continue\n",
        "  \n",
        "  emb /= len(words)\n",
        "  return emb\n",
        "\n",
        "gpt_emb = np.zeros((1000, 768))\n",
        "\n",
        "# get vectors\n",
        "\n",
        "for v in range(1000):\n",
        "    txt = df.loc[v,'question']\n",
        "    \n",
        "    gpt_emb[v] = Fetch_gpt_vectors(txt)\n",
        "\n",
        "gpt_emb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_3KRtMclniK"
      },
      "source": [
        "# Finding Similar questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9PlGLSWkyuI"
      },
      "source": [
        "#defining function to derive cosine similarity\n",
        "\n",
        "#import \n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(vec1,vec2):\n",
        "    #find the score\n",
        "    return dot(vec1, vec2)/(norm(vec1)*norm(vec2)) \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPXOcMx_kyxu"
      },
      "source": [
        "#Function which gts Top N similar questions from data \n",
        "\n",
        "def top_n_questions(user,embeddings,df):    \n",
        "    \n",
        "    #getting cosine similarities of overall data set with input queries from user\n",
        "    x=cosine_similarity(user,embeddings).tolist()[0]\n",
        "    temp_list=list(x)\n",
        "\n",
        "    #sorting\n",
        "    sort_res = sorted(range(len(x)), key = lambda sub: x[sub])[:]\n",
        "    sim_score=[temp_list[i] for i in reversed(sort_res)]\n",
        "\n",
        "    #print\n",
        "    print(sort_res[0:5])\n",
        " \n",
        "    #index fetching\n",
        "    L=[]\n",
        "    for i in reversed(sort_res):\n",
        "        L.append(i)\n",
        "\n",
        "    #get the index from dataframe \n",
        "    return df.iloc[L[0:5], [0,1]]\n",
        "\n",
        "#function to fetch the results based on the model selected\n",
        "\n",
        "def get_input_vector(query,model):    \n",
        "    \n",
        "    print(query)\n",
        "\n",
        "    #Doc2vec model\n",
        "    if model=='Doc2Vec':\n",
        "      k=fetch_embeddings(doc_model,query)\n",
        "      k=k.reshape(1, -1)\n",
        "\n",
        "    # sbert  model\n",
        "    elif model=='BERT':\n",
        "      k=sbert_model.encode(str(query))\n",
        "      k=k.reshape(1, -1)\n",
        "\n",
        "    # gpt model\n",
        "    elif model=='GPT':\n",
        "      k=Fetch_gpt_vectors(query)  \n",
        "\n",
        "    return k\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYHvQaQwnQYL"
      },
      "source": [
        "# Example 1 - Doc2vec model\n",
        "\n",
        "top_n_questions(get_input_vector('How is Narendra Modi as a person?','Doc2Vec'),doc_embeddings,df) \n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2itF45rnQcn"
      },
      "source": [
        "# Example 2 - GPT model\n",
        "\n",
        "\n",
        "top_n_questions(get_input_vector('How is Narendra Modi as a person?','GPT'),gpt_emb,df) \n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp7xEaMVnQf3"
      },
      "source": [
        "# Example 3 - BERT\n",
        "\n",
        "top_n_questions(get_input_vector('How is Narendra Modi as a person?','BERT'),sentence_embeddings_BERT,df) \n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1vCryIXnZXs"
      },
      "source": [
        "# **Implementation: Supervised Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhHszWMznbeo",
        "outputId": "2c62b327-caca-4603-a015-e9a1700fd0d9"
      },
      "source": [
        "# import packages required.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import os\n",
        "import string\n",
        "import csv\n",
        "\n",
        "#import nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#import tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "#import warnings\n",
        "import warnings\n",
        "\n",
        "#import sklearn and matplotlib\n",
        "from sklearn import preprocessing\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt \n",
        "import plotly.graph_objects as go\n",
        "\n",
        "#import warning\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n",
        "\n",
        "from string import punctuation\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#import Tokenizer from keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#importing Keras necessary libraries\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Embedding, Dense, Dropout, LSTM\n",
        "\n",
        "\n",
        "#importing train data - Import the full data\n",
        "quora_questions=pd.read_csv('Quora.csv') \n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSsExIJnby6"
      },
      "source": [
        "#function for data cleaning\n",
        "def txt_process(input_text):\n",
        "\n",
        "    # Removing punctuation from input text\n",
        "    input_text = ''.join([x for x in input_text if x not in punctuation])\n",
        "    \n",
        "    # Cleaning the text\n",
        "    input_text = re.sub(r\"[^A-Za-z0-9]\", \" \", input_text)\n",
        "    input_text = re.sub(r\"\\'s\", \" \", input_text)\n",
        "      \n",
        "    # remove stop words\n",
        "    input_text = input_text.split()\n",
        "    input_text = [x for x in input_text if not x in stop_words]\n",
        "    input_text = \" \".join(input_text)\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(input_text)\n",
        "\n",
        "\n",
        "#applying above function on both question ids\n",
        "quora_questions['question1_cleaned'] = quora_questions.apply(lambda x: txt_process(x['question1']), axis = 1)  \n",
        "quora_questions['question2_cleaned'] = quora_questions.apply(lambda x: txt_process(x['question2']), axis = 1)\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "jcC_K3TUntvC",
        "outputId": "f51348c2-1a47-40e3-d5ec-9478a1308af2"
      },
      "source": [
        "#stacking\n",
        "question_text = np.hstack([quora_questions.question1_cleaned, quora_questions.question2_cleaned])\n",
        "\n",
        "#tokenizing\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(question_text)\n",
        "\n",
        "#creating new columns for both ids where tokenized form of sentence is created \n",
        "quora_questions['tokenizer_1'] = tokenizer.texts_to_sequences(quora_questions.question1_cleaned)\n",
        "quora_questions['tokenizer_2'] = tokenizer.texts_to_sequences(quora_questions.question2_cleaned)\n",
        "\n",
        "quora_questions.head(5)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "      <th>question1_cleaned</th>\n",
              "      <th>question2_cleaned</th>\n",
              "      <th>tokenizer_1</th>\n",
              "      <th>tokenizer_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "      <td>What step step guide invest share market india</td>\n",
              "      <td>What step step guide invest share market</td>\n",
              "      <td>[1, 1054, 1054, 3819, 577, 431, 369, 9]</td>\n",
              "      <td>[1, 1054, 1054, 3819, 577, 431, 369]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "      <td>What story Kohinoor KohiNoor Diamond</td>\n",
              "      <td>What would happen Indian government stole Kohi...</td>\n",
              "      <td>[1, 325, 2313, 2313, 3820]</td>\n",
              "      <td>[1, 14, 132, 42, 133, 4595, 2313, 2313, 3820, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "      <td>How I increase speed internet connection using...</td>\n",
              "      <td>How Internet speed increased hacking DNS</td>\n",
              "      <td>[3, 2, 109, 432, 237, 1461, 84, 2960]</td>\n",
              "      <td>[3, 237, 432, 2037, 1319, 8527]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "      <td>Why I mentally lonely How I solve</td>\n",
              "      <td>Find remainder math2324math divided 2423</td>\n",
              "      <td>[4, 2, 1462, 3821, 3, 2, 578]</td>\n",
              "      <td>[37, 8528, 8529, 8530, 8531]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "      <td>Which one dissolve water quikly sugar salt met...</td>\n",
              "      <td>Which fish would survive salt water</td>\n",
              "      <td>[8, 15, 2961, 161, 5948, 1304, 1305, 5949, 130...</td>\n",
              "      <td>[8, 4258, 14, 1928, 1305, 161]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                        tokenizer_2\n",
              "0   0  ...               [1, 1054, 1054, 3819, 577, 431, 369]\n",
              "1   1  ...  [1, 14, 132, 42, 133, 4595, 2313, 2313, 3820, ...\n",
              "2   2  ...                    [3, 237, 432, 2037, 1319, 8527]\n",
              "3   3  ...                       [37, 8528, 8529, 8530, 8531]\n",
              "4   4  ...                     [8, 4258, 14, 1928, 1305, 161]\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D9ij7SNnt1k"
      },
      "source": [
        "#combining both tokens in one list question1 followed by question2\n",
        "quora_questions['tokenizer'] = quora_questions['tokenizer_1'] + quora_questions['tokenizer_2']\n",
        "\n",
        "#defining max length \n",
        "m_len = 500\n",
        "\n",
        "#max tokens\n",
        "max_token = np.max(quora_questions.tokenizer.max())\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_o1sQFCnt6q",
        "outputId": "b1d6c06f-5979-444f-d899-3403d92f5e08"
      },
      "source": [
        "#defining X and target data \n",
        "y = quora_questions[['is_duplicate']]\n",
        "X = quora_questions[['tokenizer']]\n",
        "\n",
        "#padding X with a maximum length\n",
        "X = sequence.pad_sequences(X.tokenizer, maxlen = m_len)\n",
        "\n",
        "#splitting data into train and test\n",
        "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.25, random_state=10)\n",
        "\n",
        "\n",
        "#defining the LSTM model\n",
        "quora_model = Sequential()\n",
        "\n",
        "#adding embeedding layer\n",
        "quora_model.add(Embedding(70000, 64))\n",
        "\n",
        "#adding drop out layer\n",
        "quora_model.add(Dropout(0.15))\n",
        "\n",
        "#LSTM layer\n",
        "quora_model.add(LSTM(16))\n",
        "\n",
        "#adding sigmoid layer\n",
        "quora_model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "#defining loss and optimizer\n",
        "quora_model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "quora_model.summary()\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          4480000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 16)                5184      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 4,485,201\n",
            "Trainable params: 4,485,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bp_diXint-Z",
        "outputId": "0d233ecb-a371-41e2-b431-43476e5bb216"
      },
      "source": [
        "#training the model and validating on test data\n",
        "quora_model.fit(X_train, y_train, epochs = 2, batch_size=64,validation_data=(X_test,y_test))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "59/59 [==============================] - 19s 279ms/step - loss: 0.6846 - accuracy: 0.5973 - val_loss: 0.6759 - val_accuracy: 0.6216\n",
            "Epoch 2/2\n",
            "59/59 [==============================] - 16s 271ms/step - loss: 0.6735 - accuracy: 0.6165 - val_loss: 0.6684 - val_accuracy: 0.6216\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa2dd5a3490>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP3tRk-vnuBj",
        "outputId": "abdf568e-476d-4cf8-a141-07d8503c94ee"
      },
      "source": [
        "# evaluation of the model\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#prediction on train data\n",
        "tr_prediction=quora_model.predict(X_train) \n",
        "\n",
        "#replacing probabilities >0.5 with 1 and other 0\n",
        "tr_prediction[tr_prediction>0.5]=1 \n",
        "tr_prediction[tr_prediction<0.5]=0\n",
        "tr_prediction\n",
        "\n",
        "#true values of train data\n",
        "tr_true=y_train.values\n",
        "\n",
        "#accuracy\n",
        "Accuracy=sklearn.metrics.accuracy_score(np.array(tr_true), \n",
        "                                     np.array(tr_prediction))\n",
        "\n",
        "print(Accuracy)\n",
        "0.7811906400332337\n",
        "\n",
        "#classification report with f1 score\n",
        "\n",
        "print(classification_report(tr_true, tr_prediction, target_names=['Not similar','similar']))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6165333333333334\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " Not similar       0.62      1.00      0.76      2312\n",
            "     similar       0.00      0.00      0.00      1438\n",
            "\n",
            "    accuracy                           0.62      3750\n",
            "   macro avg       0.31      0.50      0.38      3750\n",
            "weighted avg       0.38      0.62      0.47      3750\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWsmYSYinb29",
        "outputId": "c3c79d23-15f3-4a3c-8120-8a0cf8bf16c3"
      },
      "source": [
        "#predicting on test data\n",
        "test_prediction=quora_model.predict(X_test)\n",
        "\n",
        "#generating classes\n",
        "test_prediction[test_prediction>0.5]=1 \n",
        "test_prediction[test_prediction<0.5]=0\n",
        "test_prediction\n",
        "\n",
        "#true values for test\n",
        "test_true=y_test.values\n",
        "\n",
        "# accuracy on test data\n",
        "Accuracy=sklearn.metrics.accuracy_score(np.array(test_true), \n",
        "                                     np.array(test_prediction))\n",
        "\n",
        "print('Accuracy is %f'%(Accuracy*100)+' %')\n",
        "\n",
        "print(classification_report(test_true, test_prediction, target_names=['Not similar','similar']))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is 62.160000 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " Not similar       0.62      1.00      0.77       777\n",
            "     similar       0.00      0.00      0.00       473\n",
            "\n",
            "    accuracy                           0.62      1250\n",
            "   macro avg       0.31      0.50      0.38      1250\n",
            "weighted avg       0.39      0.62      0.48      1250\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB0y5S-foM44"
      },
      "source": [
        "def find_similarity_score(q1,q2):\n",
        "\n",
        "  #clean first question\n",
        "  Q1_C=  txt_process(q1) \n",
        "  #print(q1)\n",
        "\n",
        "  #clean first question\n",
        "  Q2_C = txt_process(q2)    \n",
        "  #print(q2)\n",
        "\n",
        "  #converting 1st question into tokens\n",
        "  Q1_C = tokenizer.texts_to_sequences([Q1_C])  \n",
        "\n",
        "  #converting 2nd question into token\n",
        "  Q2_C = tokenizer.texts_to_sequences([Q2_C])\n",
        "\n",
        "  #combining both tokens as we did for train data\n",
        "  Q_final = Q1_C[0] + Q2_C[0]                      \n",
        "  \n",
        "  #padding combined sequence to max length\n",
        "  Q_Test = sequence.pad_sequences([Q_final], maxlen = 500)  \n",
        "  \n",
        "  #predicting probability of given pair\n",
        "  Prob=quora_model.predict(Q_Test)         \n",
        "  print(Prob)\n",
        "\n",
        "  #if p>0.5 then similar\n",
        "  if Prob[0]>0.5:                  \n",
        "    return 'Quora Questions are similar'\n",
        "  else:\n",
        "    return 'Quora Questions are Not similar'\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b43eVBzkoN3q"
      },
      "source": [
        "#example 1\n",
        "find_similarity_score('Who is Narendra Modi?','What is identity of Narendra Modi?')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzjJHMzAoN7f"
      },
      "source": [
        "#example 2\n",
        "find_similarity_score('is there life after death?','Do people belive in afterlife')\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPARLjQVoN-u"
      },
      "source": [
        "#example 3\n",
        "find_similarity_score('Should I have a hair transplant at age 24? How much would it cost?','How much cost does hair transplant require?')\n"
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}